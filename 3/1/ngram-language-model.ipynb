{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение N-gram языковой модели с BPE токенизатором\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:19:51.691736Z",
     "iopub.status.busy": "2025-12-21T21:19:51.691175Z",
     "iopub.status.idle": "2025-12-21T21:19:51.721690Z",
     "shell.execute_reply": "2025-12-21T21:19:51.720909Z",
     "shell.execute_reply.started": "2025-12-21T21:19:51.691707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import regex\n",
    "\n",
    "# Для воспроизводимости\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. BPE Токенизатор\n",
    "\n",
    "Используем токенизатор из предыдущего задания с паттерном претокенизации GPT-4 (cl100k_base)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:19:51.723562Z",
     "iopub.status.busy": "2025-12-21T21:19:51.723119Z",
     "iopub.status.idle": "2025-12-21T21:19:51.759934Z",
     "shell.execute_reply": "2025-12-21T21:19:51.759114Z",
     "shell.execute_reply.started": "2025-12-21T21:19:51.723528Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class BPETokenizer:    \n",
    "    PRETOKENIZE_PATTERN = regex.compile(\n",
    "        r\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}|\\ ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\",\n",
    "        regex.UNICODE\n",
    "    )\n",
    "    \n",
    "    def __init__(self, vocab_size: int = 10000, special_tokens: List[str] = None):\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        if special_tokens is None:\n",
    "            special_tokens = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\", \"<MASK>\"]\n",
    "        self.special_tokens = special_tokens\n",
    "        \n",
    "        self.vocab: Dict[str, int] = {}\n",
    "        self.inverse_vocab: Dict[int, str] = {}\n",
    "        self.merges: List[Tuple[str, str]] = []\n",
    "        self.merge_ranks: Dict[Tuple[str, str], int] = {}  # Для быстрого поиска\n",
    "        self.cache: Dict[str, List[int]] = {}  # Кэш токенизации\n",
    "        \n",
    "        self._init_base_vocab()\n",
    "        \n",
    "    def _init_base_vocab(self):\n",
    "        for i, token in enumerate(self.special_tokens):\n",
    "            self.vocab[token] = i\n",
    "            self.inverse_vocab[i] = token\n",
    "        \n",
    "        offset = len(self.special_tokens)\n",
    "        for i in range(256):\n",
    "            byte_token = bytes([i]).decode('latin-1')\n",
    "            self.vocab[byte_token] = offset + i\n",
    "            self.inverse_vocab[offset + i] = byte_token\n",
    "            \n",
    "    def _pretokenize(self, text: str) -> List[str]:\n",
    "        tokens = self.PRETOKENIZE_PATTERN.findall(text)\n",
    "        return tokens if tokens else list(text)\n",
    "    \n",
    "    def _text_to_bytes(self, text: str) -> Tuple[str, ...]:\n",
    "        return tuple(bytes([b]).decode('latin-1') for b in text.encode('utf-8'))\n",
    "    \n",
    "    def _build_merge_ranks(self):\n",
    "        self.merge_ranks = {merge: i for i, merge in enumerate(self.merges)}\n",
    "    \n",
    "    def train(self, texts: List[str], verbose: bool = True):\n",
    "        if verbose:\n",
    "            print(\"Претокенизация и подсчет частот...\")\n",
    "        \n",
    "        word_freqs = Counter()\n",
    "        for text in tqdm(texts, disable=not verbose):\n",
    "            pretokens = self._pretokenize(text)\n",
    "            for pretoken in pretokens:\n",
    "                byte_seq = self._text_to_bytes(pretoken)\n",
    "                word_freqs[byte_seq] += 1\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Уникальных претокенов: {len(word_freqs)}\")\n",
    "            print(\"Построение индекса пар...\")\n",
    "        \n",
    "        pair_freqs = Counter()\n",
    "        pair_to_words = defaultdict(set)\n",
    "        word_to_pairs = {}\n",
    "        \n",
    "        for word in word_freqs:\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            pairs_in_word = []\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pair_freqs[pair] += word_freqs[word]\n",
    "                pair_to_words[pair].add(word)\n",
    "                pairs_in_word.append((i, pair))\n",
    "            word_to_pairs[word] = pairs_in_word\n",
    "        \n",
    "        num_merges = self.vocab_size - len(self.vocab)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Выполняем {num_merges} слияний...\")\n",
    "        \n",
    "        pbar = tqdm(range(num_merges), disable=not verbose)\n",
    "        \n",
    "        for _ in pbar:\n",
    "            if not pair_freqs:\n",
    "                break\n",
    "            \n",
    "            best_pair = pair_freqs.most_common(1)[0][0]\n",
    "            best_freq = pair_freqs[best_pair]\n",
    "            \n",
    "            if best_freq == 0:\n",
    "                break\n",
    "            \n",
    "            new_token = best_pair[0] + best_pair[1]\n",
    "            new_id = len(self.vocab)\n",
    "            self.vocab[new_token] = new_id\n",
    "            self.inverse_vocab[new_id] = new_token\n",
    "            self.merges.append(best_pair)\n",
    "            \n",
    "            pbar.set_description(f\"Freq={best_freq}, {best_pair[0]!r}+{best_pair[1]!r}\")\n",
    "            \n",
    "            affected_words = list(pair_to_words[best_pair])\n",
    "            del pair_to_words[best_pair]\n",
    "            del pair_freqs[best_pair]\n",
    "            \n",
    "            for old_word in affected_words:\n",
    "                if old_word not in word_freqs:\n",
    "                    continue\n",
    "                    \n",
    "                freq = word_freqs[old_word]\n",
    "                \n",
    "                if old_word in word_to_pairs:\n",
    "                    for pos, pair in word_to_pairs[old_word]:\n",
    "                        if pair in pair_freqs:\n",
    "                            pair_freqs[pair] -= freq\n",
    "                            if pair_freqs[pair] <= 0:\n",
    "                                del pair_freqs[pair]\n",
    "                        if pair in pair_to_words:\n",
    "                            pair_to_words[pair].discard(old_word)\n",
    "                \n",
    "                new_word = []\n",
    "                i = 0\n",
    "                word_list = list(old_word)\n",
    "                while i < len(word_list):\n",
    "                    if i < len(word_list) - 1 and word_list[i] == best_pair[0] and word_list[i + 1] == best_pair[1]:\n",
    "                        new_word.append(new_token)\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_word.append(word_list[i])\n",
    "                        i += 1\n",
    "                \n",
    "                new_word = tuple(new_word)\n",
    "                \n",
    "                del word_freqs[old_word]\n",
    "                if old_word in word_to_pairs:\n",
    "                    del word_to_pairs[old_word]\n",
    "                \n",
    "                word_freqs[new_word] += freq\n",
    "                \n",
    "                if len(new_word) >= 2:\n",
    "                    pairs_in_word = []\n",
    "                    for i in range(len(new_word) - 1):\n",
    "                        pair = (new_word[i], new_word[i + 1])\n",
    "                        pair_freqs[pair] += freq\n",
    "                        pair_to_words[pair].add(new_word)\n",
    "                        pairs_in_word.append((i, pair))\n",
    "                    word_to_pairs[new_word] = pairs_in_word\n",
    "        \n",
    "        # Индекс для быстрого encode\n",
    "        self._build_merge_ranks()\n",
    "        self.cache.clear()\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Финальный размер словаря: {len(self.vocab)}\")\n",
    "    \n",
    "    def _encode_word(self, word: str) -> List[int]:\n",
    "        \"\"\"Кодирование одного слова с кэшированием.\"\"\"\n",
    "        if word in self.cache:\n",
    "            return self.cache[word]\n",
    "        \n",
    "        tokens = list(self._text_to_bytes(word))\n",
    "        \n",
    "        if len(tokens) <= 1:\n",
    "            result = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens]\n",
    "            self.cache[word] = result\n",
    "            return result\n",
    "        \n",
    "        # BPE через приоритетную очередь\n",
    "        while len(tokens) > 1:\n",
    "            min_rank = float('inf')\n",
    "            min_idx = -1\n",
    "            \n",
    "            for i in range(len(tokens) - 1):\n",
    "                pair = (tokens[i], tokens[i + 1])\n",
    "                if pair in self.merge_ranks:\n",
    "                    rank = self.merge_ranks[pair]\n",
    "                    if rank < min_rank:\n",
    "                        min_rank = rank\n",
    "                        min_idx = i\n",
    "            \n",
    "            if min_idx == -1:\n",
    "                break\n",
    "            \n",
    "            tokens = tokens[:min_idx] + [tokens[min_idx] + tokens[min_idx + 1]] + tokens[min_idx + 2:]\n",
    "        \n",
    "        result = [self.vocab.get(t, self.vocab[\"<UNK>\"]) for t in tokens]\n",
    "        \n",
    "        if len(self.cache) < 100000:\n",
    "            self.cache[word] = result\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def encode(self, text: str, add_special_tokens: bool = False) -> List[int]:\n",
    "        tokens = []\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.vocab[\"<BOS>\"])\n",
    "        \n",
    "        pretokens = self._pretokenize(text)\n",
    "        \n",
    "        for pretoken in pretokens:\n",
    "            tokens.extend(self._encode_word(pretoken))\n",
    "        \n",
    "        if add_special_tokens:\n",
    "            tokens.append(self.vocab[\"<EOS>\"])\n",
    "            \n",
    "        return tokens\n",
    "    \n",
    "    def encode_batch(self, texts: List[str], add_special_tokens: bool = False, \n",
    "                     num_workers: int = 4) -> List[List[int]]:\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        \n",
    "        def encode_single(text):\n",
    "            return self.encode(text, add_special_tokens)\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            results = list(executor.map(encode_single, texts))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        byte_tokens = []\n",
    "        for token_id in token_ids:\n",
    "            if token_id in self.inverse_vocab:\n",
    "                token = self.inverse_vocab[token_id]\n",
    "                if token not in self.special_tokens:\n",
    "                    byte_tokens.append(token)\n",
    "        \n",
    "        byte_string = ''.join(byte_tokens).encode('latin-1')\n",
    "        return byte_string.decode('utf-8', errors='replace')\n",
    "    \n",
    "    def get_vocab_size(self) -> int:\n",
    "        return len(self.vocab)\n",
    "    \n",
    "    @property\n",
    "    def pad_token_id(self) -> int:\n",
    "        return self.vocab[\"<PAD>\"]\n",
    "    \n",
    "    @property\n",
    "    def bos_token_id(self) -> int:\n",
    "        return self.vocab[\"<BOS>\"]\n",
    "    \n",
    "    @property\n",
    "    def eos_token_id(self) -> int:\n",
    "        return self.vocab[\"<EOS>\"]\n",
    "    \n",
    "    def save(self, path: str):\n",
    "        data = {\n",
    "            'vocab': self.vocab,\n",
    "            'merges': self.merges,\n",
    "            'special_tokens': self.special_tokens\n",
    "        }\n",
    "        with open(path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path: str) -> 'BPETokenizer':\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        tokenizer = cls(vocab_size=len(data['vocab']), special_tokens=data['special_tokens'])\n",
    "        tokenizer.vocab = data['vocab']\n",
    "        tokenizer.inverse_vocab = {int(v): k for k, v in data['vocab'].items()}\n",
    "        tokenizer.merges = [tuple(m) for m in data['merges']]\n",
    "        tokenizer._build_merge_ranks()\n",
    "        return tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Подготовка обучающих данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:19:51.804944Z",
     "iopub.status.busy": "2025-12-21T21:19:51.804661Z",
     "iopub.status.idle": "2025-12-21T21:20:19.277601Z",
     "shell.execute_reply": "2025-12-21T21:20:19.276758Z",
     "shell.execute_reply.started": "2025-12-21T21:19:51.804919Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`trust_remote_code` is not supported anymore.\n",
      "Please check that the Hugging Face dataset 'wikimedia/wikipedia' isn't based on a loading script and remove `trust_remote_code`.\n",
      "If the dataset is based on a loading script, please ask the dataset author to remove it and convert it to a standard format like Parquet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка русской Википедии...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb509691b4dd43b19cf6775c814793d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e521dd6bb8a4693ad50ccb185355bf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено 0 статей...\n",
      "Загружено 10000 статей...\n",
      "Загружено 20000 статей...\n",
      "Загружено 30000 статей...\n",
      "Загружено 40000 статей...\n",
      "Загружено: 49463 статей\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "print(\"Загрузка русской Википедии...\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"wikimedia/wikipedia\",\n",
    "    \"20231101.ru\",\n",
    "    split=\"train\",\n",
    "    streaming=True, \n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "NUM_ARTICLES = 50000\n",
    "training_corpus = []\n",
    "\n",
    "for i, item in enumerate(dataset):\n",
    "    if i >= NUM_ARTICLES:\n",
    "        break\n",
    "    text = item['text']\n",
    "    if len(text) > 200:\n",
    "        training_corpus.append(text)\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Загружено {i} статей...\")\n",
    "\n",
    "print(f\"Загружено: {len(training_corpus)} статей\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:20:19.279551Z",
     "iopub.status.busy": "2025-12-21T21:20:19.279094Z",
     "iopub.status.idle": "2025-12-21T21:20:19.311434Z",
     "shell.execute_reply": "2025-12-21T21:20:19.310524Z",
     "shell.execute_reply.started": "2025-12-21T21:20:19.279525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 44463\n",
      "Test: 5000\n"
     ]
    }
   ],
   "source": [
    "random.shuffle(training_corpus)\n",
    "test_texts = training_corpus[:5000]   \n",
    "train_texts = training_corpus[5000:]    \n",
    "\n",
    "print(f\"Train: {len(train_texts)}\")\n",
    "print(f\"Test: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:20:19.312858Z",
     "iopub.status.busy": "2025-12-21T21:20:19.312554Z",
     "iopub.status.idle": "2025-12-21T21:23:46.811846Z",
     "shell.execute_reply": "2025-12-21T21:23:46.810997Z",
     "shell.execute_reply.started": "2025-12-21T21:20:19.312833Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Претокенизация и подсчет частот...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:12<00:00, 164.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных претокенов: 289663\n",
      "Построение индекса пар...\n",
      "Выполняем 4739 слияний...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Freq=222, ' Ñ\\x80ÐµÑ\\x88'+'ÐµÐ½Ð¸Ñ\\x8f': 100%|██████████| 4739/4739 [03:07<00:00, 25.24it/s]                         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Финальный размер словаря: 5000\n",
      "\n",
      "Размер словаря: 5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 5000\n",
    "\n",
    "tokenizer = BPETokenizer(vocab_size=VOCAB_SIZE)\n",
    "tokenizer.train(train_texts[:2000], verbose=True) \n",
    "\n",
    "print(f\"\\nРазмер словаря: {tokenizer.get_vocab_size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:23:46.813896Z",
     "iopub.status.busy": "2025-12-21T21:23:46.813167Z",
     "iopub.status.idle": "2025-12-21T21:23:46.819574Z",
     "shell.execute_reply": "2025-12-21T21:23:46.818887Z",
     "shell.execute_reply.started": "2025-12-21T21:23:46.813866Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходный: Привет, мир! Это тестовый текст.\n",
      "Токены: [2032, 840, 49, 2441, 38, 1952, 323, 454, 3346, 2397, 51]\n",
      "Декодированный: Привет, мир! Это тестовый текст.\n",
      "Совпадение: True\n"
     ]
    }
   ],
   "source": [
    "# Проверка токенизатора\n",
    "test_text = \"Привет, мир! Это тестовый текст.\"\n",
    "encoded = tokenizer.encode(test_text)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"Исходный: {test_text}\")\n",
    "print(f\"Токены: {encoded}\")\n",
    "print(f\"Декодированный: {decoded}\")\n",
    "print(f\"Совпадение: {test_text == decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. N-gram языковая модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:23:46.821797Z",
     "iopub.status.busy": "2025-12-21T21:23:46.821532Z",
     "iopub.status.idle": "2025-12-21T21:23:46.930007Z",
     "shell.execute_reply": "2025-12-21T21:23:46.929286Z",
     "shell.execute_reply.started": "2025-12-21T21:23:46.821770Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, token_ids: List[int], seq_length: int = 128):\n",
    "        self.token_ids = token_ids\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(0, len(self.token_ids) - self.seq_length)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.token_ids[idx:idx + self.seq_length]\n",
    "        y = self.token_ids[idx + 1:idx + self.seq_length + 1]\n",
    "        return torch.tensor(x), torch.tensor(y)\n",
    "\n",
    "\n",
    "class SimpleLM(nn.Module):    \n",
    "    def __init__(self, vocab_size: int, embed_dim: int = 256, context_size: int = 8):\n",
    "        super().__init__()\n",
    "        self.context_size = context_size\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.fc = nn.Linear(embed_dim * context_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: [batch, seq_len]\n",
    "        batch_size, seq_len = x.shape\n",
    "        \n",
    "        emb = self.embedding(x)  # [batch, seq_len, embed_dim]\n",
    "        \n",
    "        # Для каждой позиции берём context_size предыдущих токенов\n",
    "        outputs = []\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - self.context_size + 1)\n",
    "            ctx = emb[:, start:i+1, :]  # [batch, ctx_len, embed_dim]\n",
    "            \n",
    "            if ctx.shape[1] < self.context_size:\n",
    "                pad_size = self.context_size - ctx.shape[1]\n",
    "                pad = torch.zeros(batch_size, pad_size, emb.shape[2], device=x.device)\n",
    "                ctx = torch.cat([pad, ctx], dim=1)\n",
    "            \n",
    "            ctx_flat = ctx.view(batch_size, -1)  # [batch, context_size * embed_dim]\n",
    "            logits = self.fc(ctx_flat)  # [batch, vocab_size]\n",
    "            outputs.append(logits)\n",
    "        \n",
    "        return torch.stack(outputs, dim=1)  # [batch, seq_len, vocab_size]\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, epochs=5, lr=0.001):\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        ppl = math.exp(avg_loss)\n",
    "        print(f\"Epoch {epoch+1}: Loss={avg_loss:.4f}, PPL={ppl:.2f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def calculate_perplexity(model, data_loader):\n",
    "    model.eval()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in data_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), y.view(-1))\n",
    "            total_loss += loss.item() * y.numel()\n",
    "            total_tokens += y.numel()\n",
    "    \n",
    "    return math.exp(total_loss / total_tokens)\n",
    "\n",
    "\n",
    "def generate(model, tokenizer, prompt: str, max_length: int = 50, temperature: float = 0.8):\n",
    "    model.eval()\n",
    "    tokens = tokenizer.encode(prompt)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            ctx = tokens[-model.context_size:]\n",
    "            x = torch.tensor([ctx]).to(device)\n",
    "            \n",
    "            logits = model(x)[0, -1, :] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            \n",
    "            next_token = torch.multinomial(probs, 1).item()\n",
    "            tokens.append(next_token)\n",
    "            \n",
    "            if next_token == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    return tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:23:46.931408Z",
     "iopub.status.busy": "2025-12-21T21:23:46.931135Z",
     "iopub.status.idle": "2025-12-21T21:24:48.265068Z",
     "shell.execute_reply": "2025-12-21T21:24:48.264325Z",
     "shell.execute_reply.started": "2025-12-21T21:23:46.931385Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизация корпуса...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5000/5000 [01:01<00:00, 81.55it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего токенов: 2,233,931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Токенизация корпуса...\")\n",
    "all_tokens = []\n",
    "for text in tqdm(train_texts[:5000]): \n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
    "    all_tokens.extend(tokens[:500]) \n",
    "\n",
    "print(f\"Всего токенов: {len(all_tokens):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:24:48.266839Z",
     "iopub.status.busy": "2025-12-21T21:24:48.266216Z",
     "iopub.status.idle": "2025-12-21T21:24:48.425282Z",
     "shell.execute_reply": "2025-12-21T21:24:48.424530Z",
     "shell.execute_reply.started": "2025-12-21T21:24:48.266813Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Батчей в эпохе: 3927\n"
     ]
    }
   ],
   "source": [
    "SEQ_LENGTH = 32 \n",
    "BATCH_SIZE = 512\n",
    "\n",
    "dataset = TextDataset(all_tokens, seq_length=SEQ_LENGTH)\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(f\"Батчей в эпохе: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:24:48.426510Z",
     "iopub.status.busy": "2025-12-21T21:24:48.426213Z",
     "iopub.status.idle": "2025-12-21T21:24:48.479208Z",
     "shell.execute_reply": "2025-12-21T21:24:48.478374Z",
     "shell.execute_reply.started": "2025-12-21T21:24:48.426486Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = SimpleLM(\n",
    "    vocab_size=tokenizer.get_vocab_size(),\n",
    "    embed_dim=128,  # Было 256\n",
    "    context_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T21:24:48.480612Z",
     "iopub.status.busy": "2025-12-21T21:24:48.480291Z",
     "iopub.status.idle": "2025-12-21T22:01:52.032222Z",
     "shell.execute_reply": "2025-12-21T22:01:52.031277Z",
     "shell.execute_reply.started": "2025-12-21T21:24:48.480579Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/3: 100%|██████████| 3927/3927 [12:26<00:00,  5.26it/s, loss=2.9447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=3.1030, PPL=22.26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/3: 100%|██████████| 3927/3927 [12:25<00:00,  5.27it/s, loss=2.8895]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=2.9459, PPL=19.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_loader, epochs=3, lr=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Оценка модели (Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:01:52.033657Z",
     "iopub.status.busy": "2025-12-21T22:01:52.033230Z",
     "iopub.status.idle": "2025-12-21T22:06:24.780325Z",
     "shell.execute_reply": "2025-12-21T22:06:24.779537Z",
     "shell.execute_reply.started": "2025-12-21T22:01:52.033634Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train PPL: 17.43\n",
      "Val PPL: 18.33\n"
     ]
    }
   ],
   "source": [
    "train_ppl = calculate_perplexity(model, train_loader)\n",
    "val_ppl = calculate_perplexity(model, val_loader)\n",
    "print(f\"Train PPL: {train_ppl:.2f}\")\n",
    "print(f\"Val PPL: {val_ppl:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Генерация текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:06:24.781596Z",
     "iopub.status.busy": "2025-12-21T22:06:24.781367Z",
     "iopub.status.idle": "2025-12-21T22:06:25.715777Z",
     "shell.execute_reply": "2025-12-21T22:06:25.714948Z",
     "shell.execute_reply.started": "2025-12-21T22:06:24.781575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Машинное обучение → Машинное обучение в Оренбург Монахельских (пах-Испийской) Гунге (Ге союз Турклось до недели власть его преемирались по лускому.\n",
      "\n",
      "Первому причасточелю в производстве науч\n",
      "\n",
      "Россия — это → Россия — это ученика).\n",
      "\n",
      "Распространение \n",
      "Согласно данной сказке близокорд всё более расслоны. Ноус серьёзная рёка. За участие в молодых месяцев в году развержаться арестовавись с приказал, которая\n",
      "\n",
      "Нейронные сети → Нейронные сети (см. : буквовый ей. Юрьевна, проводил с ним, и убили превращения в деревню в первом столицейском районе Грана Берклиана, критер, помимик, и\n"
     ]
    }
   ],
   "source": [
    "for prompt in [\"Машинное обучение\", \"Россия — это\", \"Нейронные сети\"]:\n",
    "    print(f\"\\n{prompt} → {generate(model, tokenizer, prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Сводка\n",
    "\n",
    "### Токенизатор (BPE)\n",
    "| Параметр | Значение |\n",
    "|----------|----------|\n",
    "| Тип | Byte Pair Encoding (BPE) |\n",
    "| Паттерн претокенизации | GPT-4 (cl100k_base) |\n",
    "| Источник | [tiktoken](https://github.com/openai/tiktoken) |\n",
    "| Специальные токены | `<PAD>`, `<UNK>`, `<BOS>`, `<EOS>`, `<MASK>` |\n",
    "\n",
    "### Данные\n",
    "| Параметр | Значение |\n",
    "|----------|----------|\n",
    "| Корпус | Русская Википедия |\n",
    "| Датасет | `wikimedia/wikipedia, 20231101.ru` |\n",
    "| Текстов | 5,000 статей |\n",
    "| Токенов | 2,233,931 |\n",
    "\n",
    "### Модель\n",
    "| Параметр | Значение |\n",
    "|----------|----------|\n",
    "| Архитектура | SimpleLM (Embedding + Linear) |\n",
    "| Embedding dim | 128 |\n",
    "| Context size | 8 токенов |\n",
    "\n",
    "### Обучение\n",
    "| Параметр | Значение |\n",
    "|----------|----------|\n",
    "| Эпох | 3 |\n",
    "| Batch size | 512 |\n",
    "| Sequence length | 32 |\n",
    "| Learning rate | 0.002 |\n",
    "| Оптимизатор | Adam |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-21T22:06:25.717266Z",
     "iopub.status.busy": "2025-12-21T22:06:25.716828Z",
     "iopub.status.idle": "2025-12-21T22:06:25.722187Z",
     "shell.execute_reply": "2025-12-21T22:06:25.721332Z",
     "shell.execute_reply.started": "2025-12-21T22:06:25.717231Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Размер словаря:     5,000\n",
      "Параметров модели:  5,765,000\n",
      "Устройство:         cuda\n",
      "\n",
      "Train Perplexity:   17.43\n",
      "Val Perplexity:     18.33\n",
      "Ratio (Val/Train):  1.05x\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"\n",
    "Размер словаря:     {tokenizer.get_vocab_size():,}\n",
    "Параметров модели:  {sum(p.numel() for p in model.parameters()):,}\n",
    "Устройство:         {device}\n",
    "\n",
    "Train Perplexity:   {train_ppl:.2f}\n",
    "Val Perplexity:     {val_ppl:.2f}\n",
    "Ratio (Val/Train):  {val_ppl/train_ppl:.2f}x\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+BPE токенизатор успешно обучен на русском тексте\n",
    "\n",
    "+Perplexity низкий и стабильный (≈18)\n",
    "\n",
    "+Нет переобучения (Val/Train ≈ 1.05x)\n",
    "\n",
    "+Генерация на русском языке работает\n",
    "\n",
    "-Качество текста ограничено простой архитектурой и контекстом 8 токенов"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31234,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
