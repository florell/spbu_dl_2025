{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генерация анекдотов с дообученной моделью (локальный запуск)\n",
    "\n",
    "Ноутбук для экспериментов с уже дообученной моделью на локальном ПК."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.9.1\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import re\n",
    "import random\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загрузка базовой модели...\n",
      "Загрузка токенайзера...\n",
      "Загрузка LoRA...\n",
      "Модель загружена на CPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import torch\n",
    "import re\n",
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "BASE_MODEL_PATH = \"./qwen3_model\"\n",
    "LORA_PATH = \"./qwen3_jokes_lora_2\"\n",
    "\n",
    "print(\"Загрузка базовой модели...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    torch_dtype=torch.float32,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    "    low_cpu_mem_usage=True,\n",
    ").to(\"cpu\")\n",
    "\n",
    "print(\"Загрузка токенайзера...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    BASE_MODEL_PATH,\n",
    "    trust_remote_code=True,\n",
    "    local_files_only=True,\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Загрузка LoRA...\")\n",
    "model = PeftModel.from_pretrained(base_model, LORA_PATH).to(\"cpu\")\n",
    "model.eval()\n",
    "\n",
    "print(\"Модель загружена на CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Функция генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Загружено префиксов: 75\n",
      "   Генерация 100 вариантов для 75 префиксов\n",
      "   Всего: ~7500 анекдотов\n",
      "\n",
      "[1/75] Идёт мужик по лесу... → 100\n",
      "[2/75] Встречаются два друга... → 100\n",
      "[3/75] Приходят мужик в бар... → 100\n",
      "[4/75] Жена говорит мужу... → 100\n",
      "[5/75] Приходят альфа, бета и гамма в бар... → 100\n",
      "[6/75] Идёт медведь по лесу... → 100\n",
      "[7/75] Приходит мужик к врачу... → 100\n",
      "[8/75] Встречаются русский, американец и н... → 100\n",
      "[9/75] Идёт по улице девушка... → 100\n",
      "[10/75] Приходит мужик в магазин... → 100\n",
      "[11/75] Еще сто лет назад... → 100\n",
      "[12/75] Встречаются Вовочка и Петька... → 100\n",
      "[13/75] Идёт по лесу охотник... → 100\n",
      "[14/75] Я хорошо готовлю, стираю и убираю в... → 100\n",
      "[15/75] Жена спрашивает у мужа... → 100\n",
      "[16/75] Сидят в баре два друга... → 100\n",
      "[17/75] Идёт по пустыне караван... → 100\n",
      "[18/75] Приходит мужик в аптеку... → 100\n",
      "[19/75] Встречаются два программиста... → 100\n",
      "[20/75] - Послушайте, у этого парня в резюм... → 100\n",
      "[21/75] Приходит мужик в банк... → 100\n",
      "[22/75] Сидят на скамейке два пенсионера... → 100\n",
      "[23/75] Идёт по лесу грибник... → 100\n",
      "[24/75] Приходит мужик в ресторан... → 100\n",
      "[25/75] - Я дочитал учебник по теории вероя... → 100\n",
      "[26/75] Идёт по улице студент... → 100\n",
      "[27/75] Заходит студент в кофейню... → 100\n",
      "[28/75] Сидят в очереди два человека... → 100\n",
      "[29/75] Идёт по лесу шаман... → 100\n",
      "[30/75] Приходит мужик в библиотеку... → 100\n",
      "[31/75] Идёт по улице кот... → 100\n",
      "[32/75] Встречаются два математика... → 100\n",
      "[33/75] Приходит программист в бар... → 100\n",
      "[34/75] Сидит кот на клавиатуре... → 100\n",
      "[35/75] Доказывает теорему математик... → 100\n",
      "[36/75] Пишет код программист... → 100\n",
      "[37/75] Спрашивает LLM у пользователя... → 100\n",
      "[38/75] Встречаются feature engineer и data... → 100\n",
      "[39/75] Решает уравнение студент... → 100\n",
      "[40/75] Вышел новый альбом Оксимирона... → 100\n",
      "[41/75] Говорит кот хозяину... → 100\n",
      "[42/75] Простой способ остудить чай... → 100\n",
      "[43/75] Доказывает математик теорему... → 100\n",
      "[44/75] Спрашивает математик у кота... → 100\n",
      "[45/75] Пишет промпт для LLM... → 100\n",
      "[46/75] Сидит кот перед монитором... → 100\n",
      "[47/75] Объясняет математик программисту... → 100\n",
      "[48/75] Наняли команду 40 программистов... → 100\n",
      "[49/75] Идёт по крыше кот... → 100\n",
      "[50/75] В статье было написано... → 100\n",
      "[51/75] Спрашивает кот у математика... → 100\n",
      "[52/75] Думает программист о баге... → 100\n",
      "[53/75] Общается пользователь с LLM... → 100\n",
      "[54/75] Сидит кот на книге по алгоритмам... → 100\n",
      "[55/75] Пишет программист тесты... → 100\n",
      "[56/75] Решает LLM задачу по математике... → 100\n",
      "[57/75] Я прочитал книгу Пелевина... → 100\n",
      "[58/75] Встречаются два кота... → 100\n",
      "[59/75] Доказывает программист, что кот - э... → 100\n",
      "[60/75] Как часто девушки думают о... → 100\n",
      "[61/75] Примерно двадцать лет назад... → 100\n",
      "[62/75] Классический ML... → 100\n",
      "[63/75] Узнал сегодня забавный факт... → 100\n",
      "[64/75] Я хотел быть самим собой, обычным п... → 100\n",
      "[65/75] За окном шумит Сургут... → 100\n",
      "[66/75] Вообще я люблю только две вещи:... → 100\n",
      "[67/75] Хороший русский рэп... → 100\n",
      "[68/75] Одна бессмысленная ночь у телефона... → 100\n",
      "[69/75] В России запретили... → 100\n",
      "[70/75] Из характеристики:... → 100\n",
      "[71/75] Встречаются overfitting и underfitt... → 100\n",
      "[72/75] Идёт по дому кошка... → 100\n",
      "[73/75] Я из тех людей... → 100\n",
      "[74/75] - Я нормальный.... → 100\n",
      "[75/75] Есть только одна система:... → 100\n",
      "\n",
      "Готово! Файлы в jokes_by_prefix/\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import re\n",
    "import os\n",
    "\n",
    "prefixes = []\n",
    "with open(\"prefixes.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            parts = line.split(' ', 1)\n",
    "            if len(parts) > 1 and parts[0].isdigit():\n",
    "                prefixes.append(parts[1])\n",
    "            else:\n",
    "                prefixes.append(line)\n",
    "\n",
    "print(f\"Загружено префиксов: {len(prefixes)}\")\n",
    "\n",
    "def generate_for_prefix(prefix, max_tokens=60):\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"Закончи анекдот: {prefix} /no_think\"}\n",
    "    ]\n",
    "    \n",
    "    input_text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    torch.manual_seed(random.randint(0, 1000000))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_tokens,\n",
    "            do_sample=True,\n",
    "            temperature=random.uniform(0.7, 0.9),\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.3,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    ending = tokenizer.decode(new_tokens, skip_special_tokens=True).strip()\n",
    "    \n",
    "    for end_char in ['.', '!', '?']:\n",
    "        idx = ending.find(end_char)\n",
    "        if 0 < idx < 100:\n",
    "            ending = ending[:idx+1]\n",
    "            break\n",
    "    \n",
    "    ending = re.sub(r'<think>.*?</think>', '', ending, flags=re.DOTALL).strip()\n",
    "    ending = ending.split('\\n')[0].strip()\n",
    "    \n",
    "    return f\"{prefix} {ending}\"\n",
    "\n",
    "def safe_filename(prefix):\n",
    "    name = prefix[:40].strip()\n",
    "    name = re.sub(r'[^\\w\\s-]', '', name)\n",
    "    name = re.sub(r'\\s+', '_', name)\n",
    "    return name\n",
    "\n",
    "os.makedirs(\"jokes_by_prefix\", exist_ok=True)\n",
    "\n",
    "VARIANTS_PER_PREFIX = 100\n",
    "\n",
    "print(f\"   Генерация {VARIANTS_PER_PREFIX} вариантов для {len(prefixes)} префиксов\")\n",
    "print(f\"   Всего: ~{VARIANTS_PER_PREFIX * len(prefixes)} анекдотов\\n\")\n",
    "\n",
    "for i, prefix in enumerate(prefixes):\n",
    "    filename = f\"jokes_by_prefix/{i+1:02d}_{safe_filename(prefix)}.txt\"\n",
    "    \n",
    "    jokes = set()\n",
    "    attempts = 0\n",
    "    \n",
    "    while len(jokes) < VARIANTS_PER_PREFIX and attempts < VARIANTS_PER_PREFIX * 3:\n",
    "        joke = generate_for_prefix(prefix)\n",
    "        if len(joke) > len(prefix) + 10:\n",
    "            jokes.add(joke)\n",
    "        attempts += 1\n",
    "    \n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"# Префикс: {prefix}\\n\")\n",
    "        f.write(f\"# Вариантов: {len(jokes)}\\n\\n\")\n",
    "        for j, joke in enumerate(jokes, 1):\n",
    "            f.write(f\"{j}. {joke}\\n\\n\")\n",
    "    \n",
    "    print(f\"[{i+1}/{len(prefixes)}] {prefix[:35]}... → {len(jokes)}\")\n",
    "\n",
    "print(f\"\\nГотово! Файлы в jokes_by_prefix/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Справка по параметрам\n",
    "\n",
    "| Параметр | Диапазон | Эффект |\n",
    "|----------|----------|--------|\n",
    "| `temperature` | 0.5-1.2 | Ниже = предсказуемо, выше = креативно |\n",
    "| `top_p` | 0.7-0.95 | Nucleus sampling, ниже = консервативнее |\n",
    "| `top_k` | 20-100 | Ограничивает выбор токенов |\n",
    "| `repetition_penalty` | 1.0-1.5 | Выше = меньше повторов |\n",
    "| `max_tokens` | 100-300 | Максимальная длина |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_hw_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
