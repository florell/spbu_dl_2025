{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Дообучение Qwen3-0.6B для генерации русских анекдотов\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:44:02.004313Z",
     "iopub.status.busy": "2025-12-14T18:44:02.004044Z",
     "iopub.status.idle": "2025-12-14T18:44:02.010536Z",
     "shell.execute_reply": "2025-12-14T18:44:02.010027Z",
     "shell.execute_reply.started": "2025-12-14T18:44:02.004289Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# используем только 1 GPU чтобы избежать проблем с DataParallel\n",
    "# т к на кагле 2 карты t4 дают, меньше нельзя\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:44:02.044157Z",
     "iopub.status.busy": "2025-12-14T18:44:02.043804Z",
     "iopub.status.idle": "2025-12-14T18:45:44.388671Z",
     "shell.execute_reply": "2025-12-14T18:45:44.387794Z",
     "shell.execute_reply.started": "2025-12-14T18:44:02.044129Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m517.4/517.4 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m82.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers peft trl accelerate datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:45:44.390516Z",
     "iopub.status.busy": "2025-12-14T18:45:44.390291Z",
     "iopub.status.idle": "2025-12-14T18:45:49.046093Z",
     "shell.execute_reply": "2025-12-14T18:45:49.045460Z",
     "shell.execute_reply.started": "2025-12-14T18:45:44.390491Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch: 2.6.0+cu124\n",
      "CUDA: True\n",
      "GPU: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:45:49.047103Z",
     "iopub.status.busy": "2025-12-14T18:45:49.046758Z",
     "iopub.status.idle": "2025-12-14T18:46:35.448355Z",
     "shell.execute_reply": "2025-12-14T18:46:35.447421Z",
     "shell.execute_reply.started": "2025-12-14T18:45:49.047083Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-14 18:45:59.341894: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765737959.737790      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765737959.852295      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a7002cef5b4e02928671c0098617b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/726 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "968b81d7b7884ec59114a3b6c26396cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82fbc6e4f7d4f209a589dca3d7bc8c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41bc8ec718ac48549357e9d7d681fcc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "707e208f4d63489eab5f4c9f90a97028",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e5b8b84993477d80f3d87714ae6800",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5d9dabf76a642b69ae53c4db3b61d13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Модель загружена\n",
      "Memory: 1.19 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "MODEL_NAME = \"Qwen/Qwen3-0.6B\"\n",
    "\n",
    "# загрузка без квантизации - 0.6B влезет в T4\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"cuda:0\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Модель загружена\")\n",
    "print(f\"Memory: {torch.cuda.memory_allocated()/1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:46:35.449752Z",
     "iopub.status.busy": "2025-12-14T18:46:35.449199Z",
     "iopub.status.idle": "2025-12-14T18:49:29.547806Z",
     "shell.execute_reply": "2025-12-14T18:49:29.546885Z",
     "shell.execute_reply.started": "2025-12-14T18:46:35.449732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Fetching 10 files:   0%|                                 | 0/10 [00:00<?, ?it/s]Downloading 'LICENSE' to '/kaggle/working/qwen3_model/.cache/huggingface/download/DhCjcNQuMpl4FL346qr3tvNUCgY=.6634c8cc3133b3848ec74b9f275acaaa1ea618ab.incomplete'\n",
      "\n",
      "LICENSE: 11.3kB [00:00, 26.1MB/s]\n",
      "Download complete. Moving file to /kaggle/working/qwen3_model/LICENSE\n",
      "Downloading 'README.md' to '/kaggle/working/qwen3_model/.cache/huggingface/download/Xn7B-BWUGOee2Y6hCZtEhtFu4BE=.a50b19e76f5274f9ec99f5a5d99873dca5bff25e.incomplete'\n",
      "Downloading '.gitattributes' to '/kaggle/working/qwen3_model/.cache/huggingface/download/wPaCkH-WbT7GsmxMKKrNZTV4nSM=.52373fe24473b1aa44333d318f578ae6bf04b49b.incomplete'\n",
      "\n",
      "README.md: 14.0kB [00:00, 36.2MB/s]\n",
      "Download complete. Moving file to /kaggle/working/qwen3_model/README.md\n",
      "\n",
      ".gitattributes: 1.57kB [00:00, 11.7MB/s]\n",
      "Download complete. Moving file to /kaggle/working/qwen3_model/.gitattributes\n",
      "Fetching 10 files: 100%|████████████████████████| 10/10 [00:01<00:00,  8.23it/s]\n",
      "/kaggle/working/qwen3_model\n",
      "  adding: qwen3_model/ (stored 0%)\n",
      "  adding: qwen3_model/LICENSE (deflated 65%)\n",
      "  adding: qwen3_model/.gitattributes (deflated 87%)\n",
      "  adding: qwen3_model/generation_config.json (deflated 44%)\n",
      "  adding: qwen3_model/.cache/ (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/ (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/.gitignore (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/ (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/tokenizer_config.json.metadata (deflated 25%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/vocab.json.lock (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/tokenizer_config.json.lock (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/model.safetensors.metadata (deflated 30%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/vocab.json.metadata (deflated 27%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/generation_config.json.lock (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/merges.txt.lock (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/README.md.lock (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/LICENSE.metadata (deflated 26%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/merges.txt.metadata (deflated 27%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/tokenizer.json.metadata (deflated 30%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/generation_config.json.metadata (deflated 25%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/config.json.metadata (deflated 23%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/.gitattributes.metadata (deflated 26%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/tokenizer.json.lock (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/LICENSE.lock (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/.gitattributes.lock (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/config.json.lock (stored 0%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/README.md.metadata (deflated 25%)\n",
      "  adding: qwen3_model/.cache/huggingface/download/model.safetensors.lock (stored 0%)\n",
      "  adding: qwen3_model/tokenizer.json (deflated 81%)\n",
      "  adding: qwen3_model/vocab.json (deflated 61%)\n",
      "  adding: qwen3_model/model.safetensors (deflated 21%)\n",
      "  adding: qwen3_model/tokenizer_config.json (deflated 84%)\n",
      "  adding: qwen3_model/config.json (deflated 49%)\n",
      "  adding: qwen3_model/merges.txt (deflated 57%)\n",
      "  adding: qwen3_model/README.md (deflated 62%)\n"
     ]
    }
   ],
   "source": [
    "!huggingface-cli download Qwen/Qwen3-0.6B --local-dir /kaggle/working/qwen3_model\n",
    "!cd /kaggle/working && zip -r qwen3_model.zip qwen3_model/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:49:29.550947Z",
     "iopub.status.busy": "2025-12-14T18:49:29.550628Z",
     "iopub.status.idle": "2025-12-14T18:49:30.341677Z",
     "shell.execute_reply": "2025-12-14T18:49:30.341074Z",
     "shell.execute_reply.started": "2025-12-14T18:49:29.550915Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 18,350,080 || all params: 614,400,000 || trainable%: 2.9867\n"
     ]
    }
   ],
   "source": [
    "# LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model.enable_input_require_grads()\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:49:30.342530Z",
     "iopub.status.busy": "2025-12-14T18:49:30.342354Z",
     "iopub.status.idle": "2025-12-14T18:49:46.378294Z",
     "shell.execute_reply": "2025-12-14T18:49:46.377571Z",
     "shell.execute_reply.started": "2025-12-14T18:49:30.342515Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21da8ce6ee1b43599ef548e8bc91b955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67567b3f290949e9bd0db9bd2a10ad39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/122M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e64618d6ff45baa25e80f4b5914269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/497596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего: 497,596\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41a90bd9759f4eed8f3687e24a9b4e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=2):   0%|          | 0/497596 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "После фильтрации: 239,882\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import re\n",
    "\n",
    "dataset = load_dataset(\"igorktech/anekdots\", split=\"train\")\n",
    "print(f\"Всего: {len(dataset):,}\")\n",
    "\n",
    "def clean_joke(text):\n",
    "    if not text or not isinstance(text, str):\n",
    "        return None\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    if len(text) < 50 or len(text) > 500:\n",
    "        return None\n",
    "    if len(re.findall('[а-яА-ЯёЁ]', text)) < len(text) * 0.5:\n",
    "        return None\n",
    "    return text\n",
    "\n",
    "def filter_fn(ex):\n",
    "    if clean_joke(ex['text']) is None:\n",
    "        return False\n",
    "    ldr = ex.get('LDR')\n",
    "    if ldr is not None and ldr < 0.55:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "filtered = dataset.filter(filter_fn, num_proc=2)\n",
    "print(f\"После фильтрации: {len(filtered):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:49:46.380327Z",
     "iopub.status.busy": "2025-12-14T18:49:46.379293Z",
     "iopub.status.idle": "2025-12-14T18:49:51.158368Z",
     "shell.execute_reply": "2025-12-14T18:49:51.157577Z",
     "shell.execute_reply.started": "2025-12-14T18:49:46.380301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучающая выборка: 15,000\n"
     ]
    }
   ],
   "source": [
    "filtered = filtered.sort('LDR', reverse=True)\n",
    "train_data = filtered.select(range(min(15000, len(filtered))))\n",
    "print(f\"Обучающая выборка: {len(train_data):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:49:51.159547Z",
     "iopub.status.busy": "2025-12-14T18:49:51.159236Z",
     "iopub.status.idle": "2025-12-14T18:49:54.556558Z",
     "shell.execute_reply": "2025-12-14T18:49:54.555893Z",
     "shell.execute_reply.started": "2025-12-14T18:49:51.159528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c10f08899f34f578a987da0ee5cfa28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Данные готовы\n",
      "Пример: <|im_start|>user\n",
      "Расскажи анекдот.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "Пришел мужик к любовнице и только легли в постель, возвращается муж. Ну любовница мужику: - прыгай в окно! А тот: -что я скамоубийца с десятого этажа прыгать. Повишу до утра утром муж уйдет. Провисел всю ночь. утро\n"
     ]
    }
   ],
   "source": [
    "def format_example(ex):\n",
    "    joke = clean_joke(ex['text']) or ex['text'].strip()\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": \"Расскажи анекдот.\"},\n",
    "        {\"role\": \"assistant\", \"content\": joke}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    return {\"text\": text}\n",
    "\n",
    "formatted_dataset = train_data.map(format_example, num_proc=2)\n",
    "print(f\"Данные готовы\")\n",
    "print(f\"Пример: {formatted_dataset[0]['text'][:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:49:54.557722Z",
     "iopub.status.busy": "2025-12-14T18:49:54.557494Z",
     "iopub.status.idle": "2025-12-14T18:49:59.937549Z",
     "shell.execute_reply": "2025-12-14T18:49:59.936900Z",
     "shell.execute_reply.started": "2025-12-14T18:49:54.557698Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "587c70c5894d4bfd89d04b092977a7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/15000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Токенизация завершена\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Токенизация датасета\n",
    "def tokenize_fn(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_fn,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset.column_names,\n",
    "    num_proc=2,\n",
    ")\n",
    "\n",
    "print(f\"Токенизация завершена\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:49:59.938754Z",
     "iopub.status.busy": "2025-12-14T18:49:59.938474Z",
     "iopub.status.idle": "2025-12-14T18:49:59.996439Z",
     "shell.execute_reply": "2025-12-14T18:49:59.995717Z",
     "shell.execute_reply.started": "2025-12-14T18:49:59.938732Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainer готов\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False,\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    \n",
    "    num_train_epochs=4,          \n",
    "    learning_rate=1e-4,           \n",
    "    \n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",       \n",
    "    \n",
    "    optim=\"adamw_torch\",\n",
    "    warmup_steps=100,            \n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,          \n",
    "    max_grad_norm=0.3,         \n",
    "    \n",
    "    report_to=\"none\",\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Trainer готов\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T18:49:59.997538Z",
     "iopub.status.busy": "2025-12-14T18:49:59.997214Z",
     "iopub.status.idle": "2025-12-14T21:57:12.416490Z",
     "shell.execute_reply": "2025-12-14T21:57:12.415881Z",
     "shell.execute_reply.started": "2025-12-14T18:49:59.997512Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Начинаем обучение...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3752' max='3752' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3752/3752 3:07:07, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.127200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.384900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.301000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.242600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.237800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.232900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.197800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.218100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.248600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.239900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.188100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>2.180600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.185300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>2.170200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>2.183800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.160600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>2.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>2.100800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.046900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>2.100700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.095500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>2.070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.054400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>2.058100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.053400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>2.087300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.070800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>2.036900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.060800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>2.076700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.079600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>2.063700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.043300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>2.067700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.020700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.979100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.968000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.958300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.987600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.972000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.977000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.971700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.948800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.947000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.985900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.943400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.960900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.975700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.965300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.947900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.935900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.912200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.921400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.917800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.907700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.890300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.908600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.892000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.899300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.902000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.893700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.885100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.883100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.948800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.901300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.917100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.930400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.912300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.926500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Обучение завершено!\n"
     ]
    }
   ],
   "source": [
    "print(\"Начинаем обучение...\")\n",
    "trainer.train()\n",
    "print(\"\\nОбучение завершено!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Генерация анекдотов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-14T21:57:12.417578Z",
     "iopub.status.busy": "2025-12-14T21:57:12.417290Z",
     "iopub.status.idle": "2025-12-14T21:59:20.390294Z",
     "shell.execute_reply": "2025-12-14T21:59:20.389651Z",
     "shell.execute_reply.started": "2025-12-14T21:57:12.417551Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Тестовые анекдоты:\n",
      "============================================================\n",
      "\n",
      "1. Если у тебя нет денег, ты можешь работать на дураках с их роскошью и жить в магазине. Если же у тебя много денег - выигрываешь миллион за то, чтобы не поздно отдавать деньги! Вот почему я хочу стать финансовым борём. Наверное, есть ещё какие-то методы? Я такая честная лояльность, что если мне посоветовать один такой метод, сразу прибегаю к тому, который уже известен. Ну вот сейчас, какую идею вы хотите? А какая твой цель? (p.s. это можно было сделать без денег!) Дайте ответ!!!) (p.p.s.) Сделайте этот вопрос так, чтобы он был бы более нелепо...) Что делать, когда вы хотите заблокировать компьютер, но не можете найти правильную кнопку?...\n",
      "\n",
      "2. Вопрос: Сколько пять-шесть лет в космосе, а сто лет на Земле? Ответ: 500 лет. Всё равно, не надо туда бегать! Материя всё ещё находится на своем месте... Или же все время оставаться в живых и жить на своём месте... Действительно, везде! А вот наверно можно и туда попадать... Так что если ты думаешь, что мы должны пребывать на Земле, то у меня есть подумал... Но это уже совсем другое дело! (Извеняю) [c]AlexAlexArel1986 - Россиянин[...]Алгаторфус - алгоритм, который считает число с помощью компьютера (или аналогично). Опять - опять,\n",
      "\n",
      "3. - А вы, ну, у меня не такая жопа... - Неужели? Да вот я с тобой в одиночку пила на 13 лет!.. - Ну и тогда ты самому думай!! Ты что же не знаешь: кого больше любит один человек?.. - Мое? Нет, я знаю!!! Правда, мне никто не понравился!!! - И как это вы говорите?! Странно, почему у тебя не было никоим другом??? - Хм... Так же он был! Кто бы еще мог его заменить?? Я все время думаю: \"Он чего ни делал?\"....\"Какая мыла?!\"....Когда он захотел ездить по лесам, то я всегда шепчу ему под голову: \"Ты что делаешь? Что ж хочешь!\"....А\n",
      "\n",
      "4. Вопрос у дурака: - Какое слово можно вставить между \"Картона\" и \"Сенсуса\", чтобы получилось \"Картон-Сенссус\"? Ответ: \"Дырка\". (C)AlexA.Demidov 2013... Примерно так описано в нашем учебнике по истории. Слово, написанное над левым членом, заменяет его на \"Дерева\"... А над правым - \"Столяр\"... Так вот и все! Или \"Тактические\"/\"Политические\"...? ...Или \"Миллер\"...? Много людей удивляется тому, почему именно это слова вместо \"Корпорация\"-той же речью говорят! В чем дело?....!!! (C)AlexA.Demid\n",
      "\n",
      "5. Когда-нибудь я понял, что бедность — это когда у тебя не хватает денег на кухню, а ты живешь в магазине. Мужество — это когда ты выхожу за товары из магазина и приходишь домой с твоей накопительной сумкой... (C)Alexa@A2018 Что-то интересное? Абсолютно нет! Но если вы его сейчас посмотрите - это уже третий день без еды!!!... Или просто россиянам дали грипп???????!! ДА НЕПОДГОТОВЛЕНАЯ ЗАБРОЙ !!!!!! ВЕРНО БЫЛА ТОЛЬКО ПРИСТРЕЧКА С ДУМАМИ !!!! ГОСТИ ОТЛИЧНЫХ ЦЕН ДАЮ\n",
      "\n",
      "6. - Что вы на свете, - спрашивает мужик у жены. Жена: - Давайте поздравляем! Праздник будет в твоей археологии... Муж (улыбаясь): - Да давай поздравляю их со своей первой датой рождения... И так до того времени как я поспел и понял что я уже не женщина... - А чо это? Как тогда можно себе под ноги писать?! - Видимо тебе нет места! Ты же видишь как они сидят в зале вариться, потом обсуждают профессию, молча грудь резвится... Я их познакомился только после этого!..!! - Так почему ты ни раз говорил \"поздравление\"? - Ой, мы ведь еще два года ещё не п\n",
      "\n",
      "7. Мужик спрашивает у женщины: - Дорогая, а можно я тебе подарю свою девушку? Она подумала и отвернулась в сторону... Мужик пытается развлечься и говорит: \"Не бойся, она тебя уже неприятно обидит\". Женщина сжигает глаза и говорит: \"Нет! Я бы не хотел того!\" - А ты чего? - Ну, что-то я помню, когда мы вместе были дома, мне было плохо, и я хотел ее исполнить...\" - Но это же твой случайный выбор! - Так вот почему я люблю ее больше всех....!!! - Смотри, это все для меня!! - Вроде бы так, но она может быть даже лучше моей жены!!! - Извините, но я понимаю, что вы хот\n",
      "\n",
      "8. Сообщение от 20 января: Вчера в Госдуму приходил президент США, а сегодня - президент России... А на следующий день в Госдуму приходила Россия и не стала уходить... Спасибо! Президенты США и Россия теперь будут работать вместе. Надеюсь это помогло победить глобальноеоколограничие! Пожалуйста подключайте видеофильмы \"Черная шапочка\". (c)Alexa@foxnews.org [Алекса@foxnews.org] [ФООЗДЕЛЕНОC] (с)Alexa@foxnews.org [Alexa @ fox news] [FANZDELENO C] [COCO FANZDELO...] (d)Alexa@foxnews.org [Alexa @ Fox News] [D] [D]\n",
      "\n",
      "9. - Сынок, ты чего так грустно? - Мне приходит сосед с новыми папками и спрашивает: \"Сколько ячейки на этой бумаге?\" - Ну скажем 400, сынок! Но у меня только 100!!! - Тогда тебе не повезло! Каждый день ты должен быть на работе, чтобы понять, сколько стоит один килограмм швейцарского джинса. - А что же делать?! Завтра буду работать, а сегодня будет соседа... - Извините, сынок, мне уже давно стало трудно находить бабуля... Надо было бы просто сказать: \"У тебя только 97 ячеек...\" - Правильно! У нас 268!! Поэтому мы вместе сделали еще одну эту задачу... - Хорошо\n",
      "\n",
      "10. Если вы не знаете, что делать со своими вещами и денег, то вам стоит попробовать сделать подарок отца: снять деньги и дать ему народу. Надо будете долго ждать того момента, когда его начнут обвинять в пьянстве или грабеже... Но так уж как можно!.. Умножайте это на 10, а потом приснивайте себе возможность получить при этом поддержку близких людей. Тогда он может понять, что ты хочешь помилуйтесь ему с собой за то, что тебе все равно нужно... Не надо больше чем просто сказать: \"Спасибо!\". (C)Alexa and Yandex.Cop.2000.AlexanderAlexander Alexander Alexander Alexander Alexander Alexander Alexander Александр Алексandr Александр Альбина Alexandr Alexander Alexander Alexander Alexandre Alexander Alexandr Alexandrich Alexander Alexander\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def generate_joke(temperature=0.8, top_p=0.9):\n",
    "    messages = [{\"role\": \"user\", \"content\": \"Расскажи анекдот.\"}]\n",
    "    \n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            repetition_penalty=1.15,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )\n",
    "    \n",
    "    new_tokens = outputs[0][inputs['input_ids'].shape[1]:]\n",
    "    text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "print(\"Тестовые анекдоты:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(10):\n",
    "    print(f\"\\n{i+1}. {generate_joke()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Сохранение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-14T22:17:34.475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.save_pretrained(\"/kaggle/working/qwen3_jokes_lora\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/qwen3_jokes_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-12-14T22:17:34.475Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!cd /kaggle/working && zip -r qwen3_jokes_lora.zip qwen3_jokes_lora/"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
