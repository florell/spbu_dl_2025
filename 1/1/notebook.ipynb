{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d61b315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Tuple, Optional\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f0b655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Сигмоидная функция активации\"\"\"\n",
    "    return 1 / (1 + torch.exp(-z))\n",
    "\n",
    "def train_neuron(features: List[List[float]], \n",
    "                 labels: List[int], \n",
    "                 initial_weights: List[float], \n",
    "                 initial_bias: float, \n",
    "                 learning_rate: float, \n",
    "                 epochs: int) -> Tuple[List[float], float, List[float]]:\n",
    "    \"\"\"\n",
    "    Обучение одного нейрона с сигмоидной активацией используя градиентный спуск.\n",
    "    \n",
    "    Args:\n",
    "        features: Список векторов признаков\n",
    "        labels: Бинарные метки классов (0 или 1)\n",
    "        initial_weights: Начальные веса\n",
    "        initial_bias: Начальное смещение\n",
    "        learning_rate: Скорость обучения\n",
    "        epochs: Количество эпох\n",
    "    \n",
    "    Returns:\n",
    "        Кортеж (обновленные веса, обновленное смещение, список NLL для каждой эпохи)\n",
    "    \"\"\"\n",
    "    # Преобразуем в тензоры\n",
    "    X = torch.tensor(features, dtype=torch.float32)\n",
    "    y = torch.tensor(labels, dtype=torch.float32)\n",
    "    weights = torch.tensor(initial_weights, dtype=torch.float32)\n",
    "    bias = torch.tensor(initial_bias, dtype=torch.float32)\n",
    "    \n",
    "    n_samples = len(features)\n",
    "    nll_values = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Прямой проход\n",
    "        z = torch.matmul(X, weights) + bias  # линейная комбинация\n",
    "        y_pred = sigmoid(z)  # применяем сигмоиду\n",
    "        \n",
    "        # Вычисляем Negative Log Likelihood\n",
    "        # NLL = -1/n * sum(y*log(y_pred) + (1-y)*log(1-y_pred))\n",
    "        epsilon = 1e-7  # для численной стабильности\n",
    "        nll = -torch.mean(y * torch.log(y_pred + epsilon) + \n",
    "                         (1 - y) * torch.log(1 - y_pred + epsilon))\n",
    "        nll_values.append(round(nll.item(), 4))\n",
    "        \n",
    "        # Вычисляем градиенты\n",
    "        # dL/dy_pred = -y/y_pred + (1-y)/(1-y_pred)\n",
    "        # dy_pred/dz = y_pred * (1 - y_pred) (производная сигмоиды)\n",
    "        # dz/dw = X, dz/db = 1\n",
    "        \n",
    "        # Градиент по предсказаниям через chain rule\n",
    "        error = y_pred - y  # упрощенная форма для NLL с сигмоидой\n",
    "        \n",
    "        # Градиенты по весам и смещению\n",
    "        grad_weights = torch.matmul(X.T, error) / n_samples\n",
    "        grad_bias = torch.mean(error)\n",
    "        \n",
    "        # Обновление параметров (градиентный спуск)\n",
    "        weights = weights - learning_rate * grad_weights\n",
    "        bias = bias - learning_rate * grad_bias\n",
    "    \n",
    "    return weights.tolist(), bias.item(), nll_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "341fac1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neuron_sgd(features: List[List[float]], \n",
    "                     labels: List[int], \n",
    "                     initial_weights: List[float], \n",
    "                     initial_bias: float, \n",
    "                     learning_rate: float, \n",
    "                     epochs: int) -> Tuple[List[float], float, List[float]]:\n",
    "    \"\"\"\n",
    "    Стохастический градиентный спуск (SGD) - обновление на каждом примере.\n",
    "    \"\"\"\n",
    "    X = torch.tensor(features, dtype=torch.float32)\n",
    "    y = torch.tensor(labels, dtype=torch.float32)\n",
    "    weights = torch.tensor(initial_weights, dtype=torch.float32)\n",
    "    bias = torch.tensor(initial_bias, dtype=torch.float32)\n",
    "    \n",
    "    n_samples = len(features)\n",
    "    nll_values = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Перемешиваем данные для каждой эпохи\n",
    "        indices = list(range(n_samples))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        for idx in indices:\n",
    "            # Берем один пример\n",
    "            x_i = X[idx]\n",
    "            y_i = y[idx]\n",
    "            \n",
    "            # Прямой проход для одного примера\n",
    "            z = torch.dot(x_i, weights) + bias\n",
    "            y_pred = sigmoid(z)\n",
    "            \n",
    "            # Вычисляем loss для отслеживания\n",
    "            epsilon = 1e-7\n",
    "            loss = -(y_i * torch.log(y_pred + epsilon) + \n",
    "                    (1 - y_i) * torch.log(1 - y_pred + epsilon))\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Градиент для одного примера\n",
    "            error = y_pred - y_i\n",
    "            \n",
    "            # Обновление параметров сразу после каждого примера\n",
    "            grad_weights = x_i * error\n",
    "            grad_bias = error\n",
    "            \n",
    "            weights = weights - learning_rate * grad_weights\n",
    "            bias = bias - learning_rate * grad_bias\n",
    "        \n",
    "        # Среднее NLL за эпоху\n",
    "        avg_nll = epoch_loss / n_samples\n",
    "        nll_values.append(round(avg_nll, 4))\n",
    "    \n",
    "    return weights.tolist(), bias.item(), nll_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc29e34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_neuron_mini_batch(features: List[List[float]], \n",
    "                            labels: List[int], \n",
    "                            initial_weights: List[float], \n",
    "                            initial_bias: float, \n",
    "                            learning_rate: float, \n",
    "                            epochs: int,\n",
    "                            batch_size: int = 8) -> Tuple[List[float], float, List[float]]:\n",
    "    \"\"\"\n",
    "    Mini-batch градиентный спуск - обновление на мини-батчах.\n",
    "    \"\"\"\n",
    "    X = torch.tensor(features, dtype=torch.float32)\n",
    "    y = torch.tensor(labels, dtype=torch.float32)\n",
    "    weights = torch.tensor(initial_weights, dtype=torch.float32)\n",
    "    bias = torch.tensor(initial_bias, dtype=torch.float32)\n",
    "    \n",
    "    n_samples = len(features)\n",
    "    nll_values = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Перемешиваем данные\n",
    "        indices = list(range(n_samples))\n",
    "        random.shuffle(indices)\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Обрабатываем данные батчами\n",
    "        for i in range(0, n_samples, batch_size):\n",
    "            batch_indices = indices[i:min(i + batch_size, n_samples)]\n",
    "            X_batch = X[batch_indices]\n",
    "            y_batch = y[batch_indices]\n",
    "            \n",
    "            # Прямой проход для батча\n",
    "            z = torch.matmul(X_batch, weights) + bias\n",
    "            y_pred = sigmoid(z)\n",
    "            \n",
    "            # NLL для батча\n",
    "            epsilon = 1e-7\n",
    "            batch_loss = -torch.mean(y_batch * torch.log(y_pred + epsilon) + \n",
    "                                    (1 - y_batch) * torch.log(1 - y_pred + epsilon))\n",
    "            epoch_loss += batch_loss.item() * len(batch_indices)\n",
    "            \n",
    "            # Градиенты для батча\n",
    "            error = y_pred - y_batch\n",
    "            grad_weights = torch.matmul(X_batch.T, error) / len(batch_indices)\n",
    "            grad_bias = torch.mean(error)\n",
    "            \n",
    "            # Обновление параметров\n",
    "            weights = weights - learning_rate * grad_weights\n",
    "            bias = bias - learning_rate * grad_bias\n",
    "        \n",
    "        # Среднее NLL за эпоху\n",
    "        avg_nll = epoch_loss / n_samples\n",
    "        nll_values.append(round(avg_nll, 4))\n",
    "    \n",
    "    return weights.tolist(), bias.item(), nll_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "237d1754",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples: int = 100, \n",
    "                     n_features: int = 2, \n",
    "                     seed: Optional[int] = None) -> Tuple[List[List[float]], List[int]]:\n",
    "    \"\"\"\n",
    "    Генерация синтетического датасета для бинарной классификации.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "        torch.manual_seed(seed)\n",
    "    \n",
    "    # Генерируем два кластера\n",
    "    center1 = np.random.randn(n_features) * 2\n",
    "    center2 = np.random.randn(n_features) * 2 + 3\n",
    "    \n",
    "    features = []\n",
    "    labels = []\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        if i < n_samples // 2:\n",
    "            # Класс 0\n",
    "            point = center1 + np.random.randn(n_features) * 0.5\n",
    "            features.append(point.tolist())\n",
    "            labels.append(0)\n",
    "        else:\n",
    "            # Класс 1\n",
    "            point = center2 + np.random.randn(n_features) * 0.5\n",
    "            features.append(point.tolist())\n",
    "            labels.append(1)\n",
    "    \n",
    "    return features, labels\n",
    "\n",
    "def visualize_convergence(methods_results: dict, title: str = \"Сравнение методов оптимизации\"):\n",
    "    \"\"\"\n",
    "    Визуализация сходимости различных методов градиентного спуска.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    for method_name, nll_values in methods_results.items():\n",
    "        epochs = range(1, len(nll_values) + 1)\n",
    "        plt.plot(epochs, nll_values, marker='o', label=method_name, linewidth=2)\n",
    "    \n",
    "    plt.xlabel('Эпоха', fontsize=12)\n",
    "    plt.ylabel('NLL Loss', fontsize=12)\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def test_different_surfaces():\n",
    "    \"\"\"\n",
    "    Тестирование на различных поверхностях потерь.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ТЕСТИРОВАНИЕ НА РАЗЛИЧНЫХ ПОВЕРХНОСТЯХ ПОТЕРЬ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Параметры обучения\n",
    "    learning_rate = 0.1\n",
    "    epochs = 50\n",
    "    \n",
    "    # 1. Хорошо разделимые данные (простая поверхность)\n",
    "    print(\"\\n1. Хорошо разделимые данные (простая поверхность):\")\n",
    "    features1, labels1 = generate_dataset(100, 2, seed=42)\n",
    "    initial_weights = [0.1, -0.2]\n",
    "    initial_bias = 0.0\n",
    "    \n",
    "    results1 = {}\n",
    "    \n",
    "    # Классический градиентный спуск\n",
    "    w, b, nll = train_neuron(features1, labels1, initial_weights, \n",
    "                             initial_bias, learning_rate, epochs)\n",
    "    results1['Batch GD'] = nll\n",
    "    print(f\"Batch GD - Final NLL: {nll[-1]:.4f}\")\n",
    "    \n",
    "    # SGD\n",
    "    w, b, nll = train_neuron_sgd(features1, labels1, initial_weights, \n",
    "                                 initial_bias, learning_rate, epochs)\n",
    "    results1['SGD'] = nll\n",
    "    print(f\"SGD - Final NLL: {nll[-1]:.4f}\")\n",
    "    \n",
    "    # Mini-batch\n",
    "    w, b, nll = train_neuron_mini_batch(features1, labels1, initial_weights, \n",
    "                                        initial_bias, learning_rate, epochs, batch_size=10)\n",
    "    results1['Mini-batch (size=10)'] = nll\n",
    "    print(f\"Mini-batch - Final NLL: {nll[-1]:.4f}\")\n",
    "    \n",
    "    # 2. Плохо разделимые данные (сложная поверхность)\n",
    "    print(\"\\n2. Плохо разделимые данные (сложная поверхность):\")\n",
    "    \n",
    "    # Генерируем перекрывающиеся классы\n",
    "    np.random.seed(123)\n",
    "    features2 = []\n",
    "    labels2 = []\n",
    "    for i in range(100):\n",
    "        x = np.random.randn(2) * 2\n",
    "        # Нелинейная граница\n",
    "        if x[0]**2 + x[1]**2 + np.random.randn() * 0.5 > 2:\n",
    "            labels2.append(1)\n",
    "        else:\n",
    "            labels2.append(0)\n",
    "        features2.append(x.tolist())\n",
    "    \n",
    "    results2 = {}\n",
    "    \n",
    "    # Классический градиентный спуск\n",
    "    w, b, nll = train_neuron(features2, labels2, initial_weights, \n",
    "                             initial_bias, learning_rate * 0.5, epochs)\n",
    "    results2['Batch GD'] = nll\n",
    "    print(f\"Batch GD - Final NLL: {nll[-1]:.4f}\")\n",
    "    \n",
    "    # SGD\n",
    "    w, b, nll = train_neuron_sgd(features2, labels2, initial_weights, \n",
    "                                 initial_bias, learning_rate * 0.5, epochs)\n",
    "    results2['SGD'] = nll\n",
    "    print(f\"SGD - Final NLL: {nll[-1]:.4f}\")\n",
    "    \n",
    "    # Mini-batch\n",
    "    w, b, nll = train_neuron_mini_batch(features2, labels2, initial_weights, \n",
    "                                        initial_bias, learning_rate * 0.5, epochs, batch_size=10)\n",
    "    results2['Mini-batch (size=10)'] = nll\n",
    "    print(f\"Mini-batch - Final NLL: {nll[-1]:.4f}\")\n",
    "    \n",
    "    return results1, results2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6329dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "БАЗОВЫЙ ПРИМЕР:\n",
      "----------------------------------------\n",
      "Updated weights: [0.10698875039815903, -0.08469319343566895]\n",
      "Updated bias: -0.0335\n",
      "NLL values: [0.8006, 0.7631]\n",
      "\n",
      "============================================================\n",
      "ТЕСТ НА СИНТЕТИЧЕСКОМ ДАТАСЕТЕ (100 примеров):\n",
      "============================================================\n",
      "\n",
      "1. Классический градиентный спуск (Batch GD):\n",
      "   Final weights: [-0.5984, 1.4607]\n",
      "   Final bias: -1.4307\n",
      "   Final NLL: 0.0570\n",
      "\n",
      "2. Стохастический градиентный спуск (SGD):\n",
      "   Final weights: [-1.5463, 3.5938]\n",
      "   Final bias: -5.3089\n",
      "   Final NLL: 0.0008\n",
      "\n",
      "3. Mini-batch градиентный спуск:\n",
      "   Final weights: [-1.1499, 2.5261]\n",
      "   Final bias: -3.1924\n",
      "   Final NLL: 0.0072\n",
      "============================================================\n",
      "ТЕСТИРОВАНИЕ НА РАЗЛИЧНЫХ ПОВЕРХНОСТЯХ ПОТЕРЬ\n",
      "============================================================\n",
      "\n",
      "1. Хорошо разделимые данные (простая поверхность):\n",
      "Batch GD - Final NLL: 0.1489\n",
      "SGD - Final NLL: 0.0023\n",
      "Mini-batch - Final NLL: 0.0197\n",
      "\n",
      "2. Плохо разделимые данные (сложная поверхность):\n",
      "Batch GD - Final NLL: 0.5735\n",
      "SGD - Final NLL: 0.5334\n",
      "Mini-batch - Final NLL: 0.5238\n",
      "\n",
      "============================================================\n",
      "АНАЛИЗ РЕЗУЛЬТАТОВ:\n",
      "============================================================\n",
      "\n",
      "    1. Batch GD (классический):\n",
      "       - Стабильная сходимость\n",
      "       - Использует все данные для каждого обновления\n",
      "       - Может застревать в локальных минимумах\n",
      "    \n",
      "    2. SGD (стохастический):\n",
      "       - Более шумная сходимость\n",
      "       - Быстрее на больших датасетах\n",
      "       - Может выскакивать из локальных минимумов\n",
      "    \n",
      "    3. Mini-batch:\n",
      "       - Баланс между Batch GD и SGD\n",
      "       - Хорошая скорость и стабильность\n",
      "       - Оптимален для большинства задач\n",
      "    \n",
      "    Влияние формы поверхности:\n",
      "    - На простых поверхностях все методы сходятся хорошо\n",
      "    - На сложных поверхностях SGD может показывать лучшие результаты\n",
      "    - Mini-batch обычно дает лучший баланс скорости и качества\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"БАЗОВЫЙ ПРИМЕР:\")\n",
    "print(\"-\" * 40)\n",
    "    \n",
    "features = [[1.0, 2.0], [2.0, 1.0], [-1.0, -2.0]]\n",
    "labels = [1, 0, 0]\n",
    "initial_weights = [0.1, -0.2]\n",
    "initial_bias = 0.0\n",
    "learning_rate = 0.1\n",
    "epochs = 2\n",
    "    \n",
    "updated_weights, updated_bias, nll_values = train_neuron(\n",
    "    features, labels, initial_weights, initial_bias, learning_rate, epochs\n",
    ")\n",
    "    \n",
    "print(f\"Updated weights: {updated_weights}\")\n",
    "print(f\"Updated bias: {updated_bias:.4f}\")\n",
    "print(f\"NLL values: {nll_values}\")\n",
    "    \n",
    "# Тест на большем датасете\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ТЕСТ НА СИНТЕТИЧЕСКОМ ДАТАСЕТЕ (100 примеров):\")\n",
    "print(\"=\" * 60)\n",
    "    \n",
    "# Генерируем датасет\n",
    "features_large, labels_large = generate_dataset(100, 2, seed=42)\n",
    "initial_weights_large = [0.0, 0.0]\n",
    "initial_bias_large = 0.0\n",
    "learning_rate_large = 0.5\n",
    "epochs_large = 30\n",
    "    \n",
    "# Сравнение методов\n",
    "methods_results = {}\n",
    "    \n",
    "print(\"\\n1. Классический градиентный спуск (Batch GD):\")\n",
    "w1, b1, nll1 = train_neuron(features_large, labels_large, \n",
    "                                initial_weights_large, initial_bias_large, \n",
    "                                learning_rate_large, epochs_large)\n",
    "methods_results['Batch GD'] = nll1\n",
    "print(f\"   Final weights: {[round(w, 4) for w in w1]}\")\n",
    "print(f\"   Final bias: {b1:.4f}\")\n",
    "print(f\"   Final NLL: {nll1[-1]:.4f}\")\n",
    "    \n",
    "print(\"\\n2. Стохастический градиентный спуск (SGD):\")\n",
    "w2, b2, nll2 = train_neuron_sgd(features_large, labels_large, \n",
    "                                    initial_weights_large, initial_bias_large, \n",
    "                                    learning_rate_large, epochs_large)\n",
    "methods_results['SGD'] = nll2\n",
    "print(f\"   Final weights: {[round(w, 4) for w in w2]}\")\n",
    "print(f\"   Final bias: {b2:.4f}\")\n",
    "print(f\"   Final NLL: {nll2[-1]:.4f}\")\n",
    "    \n",
    "print(\"\\n3. Mini-batch градиентный спуск:\")\n",
    "w3, b3, nll3 = train_neuron_mini_batch(features_large, labels_large, \n",
    "                                           initial_weights_large, initial_bias_large, \n",
    "                                           learning_rate_large, epochs_large, \n",
    "                                           batch_size=10)\n",
    "methods_results['Mini-batch (size=10)'] = nll3\n",
    "print(f\"   Final weights: {[round(w, 4) for w in w3]}\")\n",
    "print(f\"   Final bias: {b3:.4f}\")\n",
    "print(f\"   Final NLL: {nll3[-1]:.4f}\")\n",
    "    \n",
    "# Тестирование на разных поверхностях\n",
    "results_simple, results_complex = test_different_surfaces()\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"АНАЛИЗ РЕЗУЛЬТАТОВ:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\"\"\n",
    "    1. Batch GD (классический):\n",
    "       - Стабильная сходимость\n",
    "       - Использует все данные для каждого обновления\n",
    "       - Может застревать в локальных минимумах\n",
    "    \n",
    "    2. SGD (стохастический):\n",
    "       - Более шумная сходимость\n",
    "       - Быстрее на больших датасетах\n",
    "       - Может выскакивать из локальных минимумов\n",
    "    \n",
    "    3. Mini-batch:\n",
    "       - Баланс между Batch GD и SGD\n",
    "       - Хорошая скорость и стабильность\n",
    "       - Оптимален для большинства задач\n",
    "    \n",
    "    Влияние формы поверхности:\n",
    "    - На простых поверхностях все методы сходятся хорошо\n",
    "    - На сложных поверхностях SGD может показывать лучшие результаты\n",
    "    - Mini-batch обычно дает лучший баланс скорости и качества\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
